/** ==========================================================================
* 2010 by KjellKod.cc. This is PUBLIC DOMAIN to use at your own risk and comes
* with no warranties. This code is yours to share, use and modify with no
* strings attached and no restrictions or obligations.
 *
 * For more information see g3log/LICENSE or refer refer to http://unlicense.org
* ============================================================================
*
* Example of a normal std::queue protected by a mutex for operations,
* making it safe for thread communication, using std::mutex from C++0x with
* the help from the std::thread library from JustSoftwareSolutions
* ref: http://www.stdthread.co.uk/doc/headers/mutex.html
*
* This exampel  was totally inspired by Anthony Williams lock-based data structures in
* Ref: "C++ Concurrency In Action" http://www.manning.com/williams */

#pragma once
#include <atomic>
#include <cstddef>
#include <thread>



template<typename Element>
class shared_queue {
 public:
   typedef char cache_line[64];
   static const std::size_t  Size = 1048576; //2^20
   enum alignas(64) { Capacity = Size + 1 };

   shared_queue() : _tail(0), _head(0) {}
   virtual ~shared_queue() {}

   void push(Element item); // pushByMOve?
   bool try_push(Element& item); // pushByMOve?

   bool try_and_pop(Element& item);
   void wait_and_pop(Element& item);
   bool pop(Element& item);

   bool empty() const;

   unsigned size() const { return 1; }
 
 private:

   size_t increment(size_t idx) const;

   cache_line _pad_tail;
   alignas(64) std::atomic <size_t>  _tail;  // tail(input) index
   // http://en.cppreference.com/w/cpp/types/aligned_storage
   //typename std::aligned_storage<sizeof(Element), alignof(Element)>::type data[Capacity];
   Element _array[Capacity];
//      Element    _array[Capacity];
   alignas(64) std::atomic<size_t>   _head; // head(output) index
};

// _buffer(reinterpret_cast<T*>(new aligned_t[_size + 1])), // need one extra element for a guard
// typedef char cache_line_pad_t[64];

// cache_line_pad_t    _pad0;
// const size_t        _size;
// const size_t        _mask;
// T* const            _buffer;

// cache_line_pad_t    _pad1;
// std::atomic<size_t> _head;

// cache_line_pad_t    _pad2;
// std::atomic<size_t> _tail;

template<typename Element>
void shared_queue<Element>::push(Element item) {
   while (false == try_push(item)) {
      std::this_thread::yield();
   }
}


template<typename Element>
bool shared_queue<Element>::try_push(Element& item) {
   const auto current_tail = _tail.load(std::memory_order_relaxed);
   const auto next_tail = increment(current_tail);
   if (next_tail != _head.load(std::memory_order_acquire)) {
      _array[current_tail] = item;
      _tail.store(next_tail, std::memory_order_release);
      return true;
   }
   return false; // full queue
}

template<typename Element>
bool try_and_pop(Element& item) {
   return pop(item);
}



template<typename Element>
void wait_and_pop(Element& item) {
   while (false == pop(item)) {
      std::this_thread::yield();
   }
}



// Pop by Consumer can only update the head (load with relaxed, store with release)
//     the tail must be accessed with at least aquire
template<typename Element>
bool shared_queue<Element>::pop(Element& item) {
   const auto current_head = _head.load(std::memory_order_relaxed);
   if (current_head == _tail.load(std::memory_order_acquire))
      return false; // empty queue

   item = _array[current_head];
   _head.store(increment(current_head), std::memory_order_release);
   return true;
}

template<typename Element>
bool shared_queue<Element>::empty() const {
   // snapshot with acceptance of that this comparison operation is not atomic
   return (_head.load() == _tail.load());
}

template<typename Element>
size_t shared_queue<Element>::increment(size_t idx) const {
   return (idx + 1) % Capacity;
}





